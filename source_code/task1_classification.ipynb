{"cells":[{"cell_type":"markdown","metadata":{"id":"sg9k_BBLMS93"},"source":["# **Deep Learning Project**\n","\n","**Eliana Battisti - Davide Dalla Stella - Francesco Trono**\n","\n","*University of Trento*\n","\n","A.Y. 2020/2021 - Deep Learning Course"]},{"cell_type":"markdown","metadata":{},"source":["Click to open in Colab:\n","\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ftrono/DL_Project/blob/main/source_code/task1_classification.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22345,"status":"ok","timestamp":1639569615515,"user":{"displayName":"Davide Dalla Stella","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07042227837747022347"},"user_tz":-60},"id":"EbSOUkq2cmbH","outputId":"0c57cac3-0fb4-466f-d03c-d84b976ecea0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0.dev20211214+cu102)\n","Collecting torch\n","  Using cached https://download.pytorch.org/whl/nightly/cu102/torch-1.11.0.dev20211215%2Bcu102-cp37-cp37m-linux_x86_64.whl (789.3 MB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0.dev20211214+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"]}],"source":["% pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KPK4NeNMS4N"},"outputs":[],"source":["import os\n","import shutil\n","import csv\n","from PIL import Image\n","import torch\n","import torchvision\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from numpy import asarray\n","from sklearn.preprocessing import MultiLabelBinarizer"]},{"cell_type":"markdown","metadata":{},"source":["**0) Set working directory & global variables**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! git clone https://github.com/ftrono/DL_Project.git\n","% cd DL_Project"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYBV9XAnR3Qt"},"outputs":[],"source":["#Global variables:\n","num_classifiers = 12\n","device = \"cuda:0\"\n","#device = \"cpu\"\n","#Main paths:\n","datapath = \"./Dataset\"\n","codepath = \"./source_code\""]},{"cell_type":"markdown","metadata":{"id":"upUeC5MRMz5N"},"source":["**1) Dataset preparation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7430,"status":"ok","timestamp":1639569626255,"user":{"displayName":"Davide Dalla Stella","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07042227837747022347"},"user_tz":-60},"id":"yaxBDddWGIBK","outputId":"13d648fc-0bd2-4cb8-cba5-fc87ebcf1833"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unzipped.\n"]}],"source":["#unzip dataset:\n","!unzip './Dataset/dataset.zip' -d . >/dev/null\n","print(\"Unzipped.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HljEsNz5PND1"},"outputs":[],"source":["#Parte training split (train / validation e validation_query, test):\n","#Creare 2 folder diverse, una per Train e l'altra per Validation, entrambe partendo dalla folder Train esistente.\n","\n","#Scaricare dataset su macchina locale (mantieni struttura sottofolder) (esempio -> https://colab.research.google.com/drive/1oOKXtUUaACKG_P1SZO-PpfygP5_12dX7#scrollTo=h8gyumMki8A6 )\n","#Parte training split (train / validation):\n","#Creare 2 folder diverse, una per Train e l'altra per Validation, entrambe partendo dalla folder Train esistente.\n","\n","#Seguire indicazioni nel TO-DOs. Lavorare tramite bash in locale:\n","#Scaricare dataset su macchina locale (mantieni struttura sottofolder) (esempio -> https://colab.research.google.com/drive/1oOKXtUUaACKG_P1SZO-PpfygP5_12dX7#scrollTo=h8gyumMki8A6 )\n","\n","def dataset_preparation(img_root, csv_dir):\n","  \n","  #PART 1\n","\n","  validation_dir = img_root+\"validation\" #directory che porta all'interno della cartella validation\n","  train_dir = img_root+\"train\" #directory che porta all'interno della cartella train\n","  n_id = -1  \n","  #contatore per sapere quanti id di persone ho. Parte da -1 così tolgo la riga che contiene solo la legenda\n","\n","  #per evitare di rischiare di ripetere lo spostamento di immagini ad ogni esecuzione del programma lo \n","  #spostamento di immagini lo eseguo solo nel caso in cui la cartella non esista così da essere sicuro di \n","  #spostare le immagini solo alla prima esecuzione\n","  if not os.path.isdir(validation_dir): #controllo se esiste la cartella validation. Se no allora la creo\n","    os.mkdir(validation_dir)\n","    #il csv_file mi serve per sapere quanti id di persone ho\n","    with open(csv_dir) as annotations_train: #conto il numero di id con un ciclo for\n","      print(annotations_train)\n","      n_id += sum(1 for row in annotations_train) #sommo il numero di righe all'interno del csv file\n","    \n","    to_move = n_id // 4\n","    \n","    print(\"Moving 25% of training files in validation\")\n","    for i in range(0,to_move):#loop che sposta le foto di un quarto delle persone contenute nella cartella\n","      #metodo per ottenere la lista coi nomi di tutti i file all'interno di una dir\n","      train_list = os.listdir(train_dir)#ad ogni ciclo aggiorno la lista così non rischio di spostare file già spostati\n","      id_to_move = train_list[0][0:4] #prendo il nome della prima immagine nella lista e mi prendo solo le prime 4 lettere che indicano l'id della persona\n","      #print(\"id to move: \", id_to_move)\n","      for files in train_list: #scorro tutta la lista dei file\n","        if files.startswith(id_to_move): #controllo se i file iniziano con l'id selezionato\n","          shutil.move(train_dir+\"/\"+files,validation_dir+\"/\"+files) #se l'id corrisponde lo sposto\n","          #print(\"Moved: \",train_dir+\"/\"+files, \"in: \",validation_dir+\"/\"+files)\n","          \n","    print(\"Training files moved to validation\")\n","  else:\n","    print(\"Validation already created.\")\n","  \n","  #PART 2\n","  \n","  '''validation_queries_dir = img_root+ \"/validation_queries\"\n","  test_dir = img_root + \"/test\"\n","\n","  if not os.path.isdir(validation_queries_dir): #Se una delle cartelle non esiste allora assumo che non ne esista nessuna\n","    if not os.path.isdir(test_dir):\n","      os.mkdir(validation_queries_dir) #creazione cartelle validation_query e test\n","      os.mkdir(test_dir)\n","      validation_list = os.listdir(validation_dir) #lista di file all'interno della cartella validation\n","      copied_id = [] #lista degli id che ho già copiato in validation list. All'inizio è vuota perchè non ho copiato nessuna immagine\n","      for file in validation_list:  #scorro la lista di file di validation\n","        id = file[0:4] #prelevo l'id del file corrente\n","        if id in copied_id: #se l'id è nell'array allora l'ho già copiato in validation_query quindi quest'altra immagine va copiata in test\n","          shutil.copy(validation_dir+\"/\"+file,test_dir+\"/\"+file) #copia file\n","        else:\n","        #se l'id non è presente nell'array allora devo copiare l'immagine in validation_query e inserire l'id nell'array dei già copiati\n","          shutil.copy(validation_dir+\"/\"+file,validation_queries_dir+\"/\"+file)\n","          copied_id.append(id)'''\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghq3W8EdTPvv"},"outputs":[],"source":["#Preparation of Dataset class (must extend standard torch Dataset class):\n","class CustomDataset(Dataset):\n","\n","  #STUDIARSI L'ESEMPIO GIA' FATTO QUI: -> https://pytorch.org/tutorials/beginner/basics/data_tutorial.html , sezione \"Creating a Custom Dataset for your files\"\n","\n","  #override:\n","  #csvfile e imgfolder sono stringhe con la directory corrispettiva\n","  def __init__(self, imgfolder, train, csvfile = None):\n","\n","    #creazione dizionario\n","    self.train = train\n","    self.imgfolder = imgfolder\n","    self.dictionary = {}\n","    self.img_list = os.listdir(imgfolder)\n","    #numero di file all'interno della cartella contenente le immagini\n","    self.size = len(self.img_list)\n","    if csvfile != None:\n","      with open(csvfile, mode='r') as annotations_train:\n","        reader = csv.reader(annotations_train, delimiter = ',')\n","        next(reader,None) #Salto prima riga per non avere i nomi degli attributi nel dizionario\n","        for row in reader:\n","          up = row[11:19] # Questo modo ritorna una sublist nel range di index indicato senza contare però l'ultimo elemento!\n","          down = row[19:]\n","          row = row[:11] # tengo per la lista solo i primi 10 attributi e quelli relativi ai colori up e down li compatto in due attributi distinti che concateno poi\n","\n","          if '2' in up: #per le label decido che il loro valore è il corrispettivo dell'index+1. Se nessun valore è ugale a '2' allora ho un multicolor che indico con un valore = all'ultima label+1\n","            row.append(up.index('2')+1) #concateno attributo upcolor\n","          else:\n","            row.append(9)\n","          if '2' in down:\n","            row.append(down.index('2')+1) #concateno attributo downcolor\n","          else:\n","            row.append(10)\n","\n","          for label in row: \n","            row[row.index(label)] = int(label) #converto in intero le label degli attributi\n","          self.dictionary[int(row.pop(0))] = row\n","\n","  #override:\n","  #return the element at index idx:\n","  def __getitem__(self, idx):\n","    img = Image.open(self.imgfolder+\"/\"+self.img_list[idx])\n","    img = T.ToTensor()(img).unsqueeze(0)\n","    if self.train == True:\n","      return img, (torch.as_tensor(self.dictionary[int(self.img_list[idx][0:4])])-1)\n","    else:\n","      return img\n","  #override:\n","  def __len__(self):\n","    #return the number of elements that compose the dataset:\n","    return self.size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNrPlJ0g2yG8"},"outputs":[],"source":["def translate_attributes(attributes):\n","  translated_attributes = {}\n","  attributes_names = [\"age\",\"backpack\",\"bag\",\"handbag\",\"clothes\",\"down\",\"up\",\"hair\",\"hat\",\"gender\",\"upcolor\",\"downcolor\"] #traduttore\n","\n","  translator = [\n","                [\"young\", \"teenager\", \"adult\", \"old\"], #index: 0 attribute: age\n","                [\"no\", \"yes\"], #index: 1 attribute: backpack\n","                [\"no\", \"yes\"], #index: 2 attribute: bag\n","                [\"no\", \"yes\"], #index: 3 attribute: handbag\n","                [\"dress\", \"pants\"], #index: 4 attribute: clothes\n","                [\"long lower body clothing\", \"short\"], #index: 5 attribute: down\n","                [\"long sleeve short\", \"sleeve\"], #index: 6 attribute: up\n","                [\"short hair\", \"long hair\"], #index: 7 attribute: hair\n","                [\"no\", \"yes\"], #index: 8 attribute: hat\n","                [\"male\",\"female\"], #index: 9 attribute: gender\n","                [\"black\", \"white\", \"red\", \"purple\", \"yellow\", \"gray\", \"blue\", \"green\", \"multicolor\"], #index: 10 attribute: upcolor\n","                [\"black\", \"white\", \"pink\", \"purple\", \"yellow\", \"gray\", \"blue\", \"green\", \"brown\", \"multicolor\"], #index: 11 attribute: downcolor\n","  ]\n","  \n","  for i in range(len(attributes)):\n","    #le posizioni degli attributi coincidono coi due vettori e per ottenere la label tradotta è sufficente prenderne il valore-1 per ottenere \n","    #l'index corrispettivo nella lista di label tradotte\n","    translated_attributes[attributes_names[i]] = translator[i][attributes[i]-1] \n","  return translated_attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9kzqHqOSMH0"},"outputs":[],"source":["#DataLoaders:\n","def get_data(batch_size, img_root):\n","\n","  # Load data:\n","  training_data = CustomDataset(img_root+\"train\", True, img_root+\"annotations_train.csv\")\n","  val_data = CustomDataset(img_root+\"validation\", True, img_root+\"annotations_train.csv\")\n","  test_data = CustomDataset(img_root+\"test\", False)\n","  query_data = CustomDataset(img_root+\"queries\", False)\n","  #print(\"tot img: \", training_data.size+val_data.size+query_data.size)\n","  # Initialize dataloaders:\n","  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=0)\n","  val_loader = torch.utils.data.DataLoader(val_data, batch_size, shuffle=False, num_workers=0)\n","  test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False, num_workers=0)\n","  query_loader = torch.utils.data.DataLoader(query_data, batch_size, shuffle=False, num_workers=0)\n","  \n","  return train_loader, val_loader, test_loader, query_loader"]},{"cell_type":"markdown","metadata":{"id":"X5jRoWUzpJ-x"},"source":["**2) CNN implementation**\n","\n","NOTA: di tutta questa parte va aggiornato completamente il codice con le nostre scelte e reimplementati tutti i classificatori!!! (per task 1 e task 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaKf3Bck6sOq"},"outputs":[],"source":["#Init chosen CNN model:\n","#Da -> https://discuss.pytorch.org/t/add-multiple-fc-layers-in-parallel/19008\n","\n","class Our_CNN(torch.nn.Module):\n","  #init override:\n","  def __init__(self, num_heads=num_classifiers, loss={'xent'}, **kwargs):\n","\n","    super(Our_CNN,self).__init__()\n","\n","    #load default resnet from pytorch:\n","    resnet = torchvision.models.resnet50(pretrained=True, progress=True)\n","    #save number of input features from last layer:\n","\n","    self.backbone = torch.nn.Sequential(*list(resnet.children())[:-1])\n","\n","\n","    self.fc0 = torch.nn.Linear(2048,4)\n","    self.fc10 = torch.nn.Linear(2048,9)\n","    self.fc11 = torch.nn.Linear(2048,10)\n","    #execute self.fc1,fc2,...,fc9 = torch.nn.Linear(2048,2)\n","    for i in range(1,10):\n","        command = \"self.fc\"+str(i)+\" = torch.nn.Linear(2048,2)\" \n","        exec(command)\n","\n","  #forward pass:\n","  def forward(self,x):\n","    #forward though backbone portion of network:\n","    x = x.reshape((-1,3,128,64))\n","    #print(x.shape)\n","    x = self.backbone(x)\n","    x = x.flatten(1)\n","\n","    for i in range(0,12):\n","      command = \"fc\"+str(i)+\" = self.fc\" + str(i)+\"(x)\"\n","      exec(command)\n","    #put the output in (batch_size, input_dim) format and save as features:\n","    feats = x.view(x.shape[0], -1)\n","\n","    #loop through fc layers and store fwd pass in outputs list:\n","    outputs = []\n","\n","    for i in range(0,12):\n","      command = \"outputs.append(fc\"+str(i)+\")\"\n","      exec(command)\n","    #print(\"fc0 device: \",fc0.device)\n","\n","    #return both output list (task 1) and features (task 2):\n","    return outputs, feats\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMhUj_Rs6v8s"},"outputs":[],"source":["#Loss function:\n","#Da -> https://colab.research.google.com/drive/1oOKXtUUaACKG_P1SZO-PpfygP5_12dX7#scrollTo=FK8A9alWqYZ2 \n","def cost_function(outputs, targets):\n","  loss = 0.0\n","  for i in range(len(outputs)):\n","    loss += F.cross_entropy(outputs[i], targets.t()[i])\n","  return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBtnyTAGszkh"},"outputs":[],"source":["#Optimizer:\n","#Da -> https://colab.research.google.com/drive/1oOKXtUUaACKG_P1SZO-PpfygP5_12dX7#scrollTo=FK8A9alWqYZ2 \n","\n","#Da aggiornare sulla base dei nostri classificatori.\n","def get_optimizer(model, lr, wd, momentum):\n","  \n","  # we will create two groups of weights, one for the newly initialized layer\n","  # and the other for rest of the layers of the network\n","  \n","  final_layer_weights = []\n","  rest_of_the_net_weights = []\n","  \n","  # we will iterate through the layers of the network\n","  for name, param in model.named_parameters():\n","    if name.startswith('classifier.6'):\n","      final_layer_weights.append(param)\n","    else:\n","      rest_of_the_net_weights.append(param)\n","  \n","  # so now we have divided the network weights into two groups.\n","  # We will train the final_layer_weights with learning_rate = lr\n","  # and rest_of_the_net_weights with learning_rate = lr / 10\n","  \n","  optimizer = torch.optim.SGD(model.parameters(),lr=lr, weight_decay=wd, momentum=momentum)\n","  '''optimizer = torch.optim.SGD([\n","      {'params': rest_of_the_net_weights},\n","      {'params': final_layer_weights, 'lr': lr}\n","  ], lr=lr / 10, weight_decay=wd, momentum=momentum)'''\n","  \n","  return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZY1hIns6zpm"},"outputs":[],"source":["#TRAIN & TEST FUNCTIONS:\n","#Da -> https://colab.research.google.com/drive/1oOKXtUUaACKG_P1SZO-PpfygP5_12dX7#scrollTo=FK8A9alWqYZ2 \n","\n","#Train function:\n","def train(net,data_loader,optimizer):\n","  samples = 0.\n","  cumulative_loss = 0.\n","  cumulative_accuracy = 0.\n","\n","  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n","  for batch_idx, (inputs, targets) in enumerate(data_loader):\n","    # Load data into GPU\n","    inputs = inputs.to(device)\n","    targets = targets.to(device)\n","    # Forward pass\n","    outputs,feats = net(inputs)\n","    loss = 0\n","\n","    loss = cost_function(outputs,targets)\n","\n","    #stats print:\n","    samples+=inputs.shape[0]\n","\n","    cumulative_loss += loss.item()\n","\n","    predicted = []\n","    for i in range(len(outputs)):\n","      predicted.append(outputs[i].max(1)[1])\n","      cumulative_accuracy += predicted[i].eq(targets.t()[i]).sum().item()\n","      \n","    # Backward pass\n","    loss.backward()\n","    \n","    # Update parameters\n","    optimizer.step()\n","    \n","    # Reset the optimizer\n","    optimizer.zero_grad()\n","  return cumulative_loss/samples, 100*cumulative_accuracy/(len(outputs)*samples)\n","\n","#Test function:\n","def test(net, data_loader):\n","  samples = 0.\n","  cumulative_loss = 0.\n","  cumulative_accuracy = 0.\n","\n","  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n","  with torch.no_grad():\n","    for batch_idx, (inputs, targets) in enumerate(data_loader):\n","\n","      #print(\"batch_idx: \",batch_idx)\n","      # Load data into GPU\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      # Forward pass\n","      outputs,feats = net(inputs)\n","      \n","      # Apply the loss:\n","      loss = 0\n","      #for i in range(inputs.shape[0]):\n","       # layer_loss = cost_function(outputs[i], targets[i])\n","        # loss = loss + layer_loss\n","      loss = cost_function(outputs,targets)\n","      samples+=inputs.shape[0]\n","      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n","      predicted = []\n","      for i in range(len(outputs)):\n","        predicted.append(outputs[i].max(1)[1])\n","        cumulative_accuracy += predicted[i].eq(targets.t()[i]).sum().item()\n","\n","  return cumulative_loss/samples, 100*cumulative_accuracy/(len(outputs)*samples)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urs7bAoxvs8c"},"outputs":[],"source":["#Funzione di esportazione file output per task 1 (\"classification_test.csv\" - vedi PDF assignment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2M8b7XKVv2xl"},"outputs":[],"source":["#Funzione di esportazione file output per task 2 (\"reid_text.txt\" - vedi PDF assignment)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2492713,"status":"ok","timestamp":1639572119380,"user":{"displayName":"Davide Dalla Stella","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07042227837747022347"},"user_tz":-60},"id":"3Dc6pe4BuhP0","outputId":"5d0a8d84-fdd3-40bb-e56d-158e8898dafc"},"outputs":[{"name":"stdout","output_type":"stream","text":["<_io.TextIOWrapper name='/content/Dataset/annotations_train.csv' mode='r' encoding='UTF-8'>\n","Moving 25% of training files in validation\n","Training files moved to validation\n","Tesla K80\n","Before training:\n","\t Training loss 0.18948, Training accuracy 44.58\n","\t Val loss 0.19033, Val accuracy 44.74\n","-----------------------------------------------------\n","Epoch: 1\n","\t Training loss 0.10261, Training accuracy 78.78\n","\t Val loss 0.09277, Val accuracy 81.90\n","-----------------------------------------------------\n","Epoch: 2\n","\t Training loss 0.05306, Training accuracy 89.70\n","\t Val loss 0.09321, Val accuracy 82.47\n","-----------------------------------------------------\n","Epoch: 3\n","\t Training loss 0.02683, Training accuracy 95.48\n","\t Val loss 0.10309, Val accuracy 83.14\n","-----------------------------------------------------\n","Epoch: 4\n","\t Training loss 0.01288, Training accuracy 98.31\n","\t Val loss 0.11202, Val accuracy 82.82\n","-----------------------------------------------------\n","Epoch: 5\n","\t Training loss 0.00659, Training accuracy 99.40\n","\t Val loss 0.11817, Val accuracy 83.19\n","-----------------------------------------------------\n","Epoch: 6\n","\t Training loss 0.00392, Training accuracy 99.74\n","\t Val loss 0.12346, Val accuracy 83.23\n","-----------------------------------------------------\n","Epoch: 7\n","\t Training loss 0.00246, Training accuracy 99.90\n","\t Val loss 0.12710, Val accuracy 83.52\n","-----------------------------------------------------\n","Epoch: 8\n","\t Training loss 0.00176, Training accuracy 99.94\n","\t Val loss 0.13082, Val accuracy 83.37\n","-----------------------------------------------------\n","Epoch: 9\n","\t Training loss 0.00128, Training accuracy 99.98\n","\t Val loss 0.13515, Val accuracy 83.28\n","-----------------------------------------------------\n","Epoch: 10\n","\t Training loss 0.00104, Training accuracy 99.99\n","\t Val loss 0.13879, Val accuracy 83.17\n","-----------------------------------------------------\n","Epoch: 11\n","\t Training loss 0.00086, Training accuracy 99.99\n","\t Val loss 0.13875, Val accuracy 83.41\n","-----------------------------------------------------\n","Epoch: 12\n","\t Training loss 0.00073, Training accuracy 99.99\n","\t Val loss 0.14191, Val accuracy 83.24\n","-----------------------------------------------------\n","Epoch: 13\n","\t Training loss 0.00063, Training accuracy 99.99\n","\t Val loss 0.14192, Val accuracy 83.30\n","-----------------------------------------------------\n","Epoch: 14\n","\t Training loss 0.00054, Training accuracy 100.00\n","\t Val loss 0.14390, Val accuracy 83.37\n","-----------------------------------------------------\n","Epoch: 15\n","\t Training loss 0.00049, Training accuracy 100.00\n","\t Val loss 0.14611, Val accuracy 83.46\n","-----------------------------------------------------\n","Epoch: 16\n","\t Training loss 0.00044, Training accuracy 100.00\n","\t Val loss 0.14690, Val accuracy 83.48\n","-----------------------------------------------------\n","Epoch: 17\n","\t Training loss 0.00038, Training accuracy 100.00\n","\t Val loss 0.14732, Val accuracy 83.56\n","-----------------------------------------------------\n","Epoch: 18\n","\t Training loss 0.00037, Training accuracy 99.99\n","\t Val loss 0.14980, Val accuracy 83.55\n","-----------------------------------------------------\n","Epoch: 19\n","\t Training loss 0.00034, Training accuracy 100.00\n","\t Val loss 0.15010, Val accuracy 83.49\n","-----------------------------------------------------\n","Epoch: 20\n","\t Training loss 0.00030, Training accuracy 100.00\n","\t Val loss 0.15031, Val accuracy 83.39\n","-----------------------------------------------------\n","Epoch: 21\n","\t Training loss 0.00031, Training accuracy 100.00\n","\t Val loss 0.15277, Val accuracy 83.36\n","-----------------------------------------------------\n","Epoch: 22\n","\t Training loss 0.00033, Training accuracy 99.99\n","\t Val loss 0.15369, Val accuracy 83.44\n","-----------------------------------------------------\n","Epoch: 23\n","\t Training loss 0.00027, Training accuracy 100.00\n","\t Val loss 0.15374, Val accuracy 83.49\n","-----------------------------------------------------\n","Epoch: 24\n","\t Training loss 0.00029, Training accuracy 100.00\n","\t Val loss 0.15531, Val accuracy 83.49\n","-----------------------------------------------------\n","Epoch: 25\n","\t Training loss 0.00028, Training accuracy 99.99\n","\t Val loss 0.15509, Val accuracy 83.61\n","-----------------------------------------------------\n","Epoch: 26\n","\t Training loss 0.00026, Training accuracy 100.00\n","\t Val loss 0.15758, Val accuracy 83.37\n","-----------------------------------------------------\n","Epoch: 27\n","\t Training loss 0.00022, Training accuracy 100.00\n","\t Val loss 0.15692, Val accuracy 83.46\n","-----------------------------------------------------\n","Epoch: 28\n","\t Training loss 0.00020, Training accuracy 100.00\n","\t Val loss 0.15700, Val accuracy 83.41\n","-----------------------------------------------------\n","Epoch: 29\n","\t Training loss 0.00020, Training accuracy 100.00\n","\t Val loss 0.15911, Val accuracy 83.50\n","-----------------------------------------------------\n","Epoch: 30\n","\t Training loss 0.00019, Training accuracy 100.00\n","\t Val loss 0.15931, Val accuracy 83.42\n","-----------------------------------------------------\n","Epoch: 31\n","\t Training loss 0.00018, Training accuracy 100.00\n","\t Val loss 0.15973, Val accuracy 83.59\n","-----------------------------------------------------\n","Epoch: 32\n","\t Training loss 0.00017, Training accuracy 100.00\n","\t Val loss 0.16071, Val accuracy 83.52\n","-----------------------------------------------------\n","Epoch: 33\n","\t Training loss 0.00016, Training accuracy 100.00\n","\t Val loss 0.16001, Val accuracy 83.49\n","-----------------------------------------------------\n","Epoch: 34\n","\t Training loss 0.00015, Training accuracy 100.00\n","\t Val loss 0.16106, Val accuracy 83.43\n","-----------------------------------------------------\n","Epoch: 35\n","\t Training loss 0.00016, Training accuracy 100.00\n","\t Val loss 0.16239, Val accuracy 83.33\n","-----------------------------------------------------\n","Epoch: 36\n","\t Training loss 0.00014, Training accuracy 100.00\n","\t Val loss 0.16209, Val accuracy 83.60\n","-----------------------------------------------------\n","Epoch: 37\n","\t Training loss 0.00014, Training accuracy 100.00\n","\t Val loss 0.16319, Val accuracy 83.61\n","-----------------------------------------------------\n","Epoch: 38\n","\t Training loss 0.00014, Training accuracy 100.00\n","\t Val loss 0.16295, Val accuracy 83.50\n","-----------------------------------------------------\n","Epoch: 39\n","\t Training loss 0.00013, Training accuracy 100.00\n","\t Val loss 0.16351, Val accuracy 83.42\n","-----------------------------------------------------\n","Epoch: 40\n","\t Training loss 0.00012, Training accuracy 100.00\n","\t Val loss 0.16368, Val accuracy 83.45\n","-----------------------------------------------------\n","Epoch: 41\n","\t Training loss 0.00011, Training accuracy 100.00\n","\t Val loss 0.16461, Val accuracy 83.58\n","-----------------------------------------------------\n","Epoch: 42\n","\t Training loss 0.00012, Training accuracy 100.00\n","\t Val loss 0.16486, Val accuracy 83.48\n","-----------------------------------------------------\n","Epoch: 43\n","\t Training loss 0.00011, Training accuracy 100.00\n","\t Val loss 0.16456, Val accuracy 83.56\n","-----------------------------------------------------\n","Epoch: 44\n","\t Training loss 0.00011, Training accuracy 100.00\n","\t Val loss 0.16490, Val accuracy 83.57\n","-----------------------------------------------------\n","Epoch: 45\n","\t Training loss 0.00010, Training accuracy 100.00\n","\t Val loss 0.16593, Val accuracy 83.52\n","-----------------------------------------------------\n","Epoch: 46\n","\t Training loss 0.00011, Training accuracy 100.00\n","\t Val loss 0.16586, Val accuracy 83.58\n","-----------------------------------------------------\n","Epoch: 47\n","\t Training loss 0.00010, Training accuracy 100.00\n","\t Val loss 0.16709, Val accuracy 83.41\n","-----------------------------------------------------\n","Epoch: 48\n","\t Training loss 0.00010, Training accuracy 100.00\n","\t Val loss 0.16677, Val accuracy 83.43\n","-----------------------------------------------------\n","Epoch: 49\n","\t Training loss 0.00009, Training accuracy 100.00\n","\t Val loss 0.16760, Val accuracy 83.33\n","-----------------------------------------------------\n","Epoch: 50\n","\t Training loss 0.00009, Training accuracy 100.00\n","\t Val loss 0.16649, Val accuracy 83.62\n","-----------------------------------------------------\n","After training:\n","\t Training loss 0.00002, Training accuracy 100.00\n","\t Val loss 0.16649, Val accuracy 83.62\n","-----------------------------------------------------\n"]}],"source":["#MAIN PER TASK 1\n","#Modificare da -> https://colab.research.google.com/drive/1oOKXtUUaACKG_P1SZO-PpfygP5_12dX7#scrollTo=FK8A9alWqYZ2 \n","#MAIN PER TASK 1\n","'''\n","Input arguments\n","  batch_size: Size of a mini-batch\n","  device: GPU where you want to train your network\n","  weight_decay: Weight decay co-efficient for regularization of weights\n","  momentum: Momentum for SGD optimizer\n","  epochs: Number of epochs for training the network\n","  num_classes: Number of classes in your dataset\n","  visualization_name: Name of the visualization folder\n","  img_root: The root folder of images\n","'''\n","\n","def main(batch_size=64, \n","         learning_rate=0.001, \n","         weight_decay=0.000001, \n","         momentum=0.9, \n","         epochs=50, \n","         num_classes=12, \n","         visualization_name='resnet50', \n","         img_root=datapath):\n","  \n","  dataset_preparation(img_root,img_root+\"annotations_train.csv\")#function that prepare the validation, query_validation and test folders\n","  writer = SummaryWriter(log_dir=\"runs/exp1\")\n","\n","  # Instantiates dataloaders\n","  train_loader, val_loader, test_loader, query_loader = get_data(batch_size=batch_size, img_root=img_root)\n","  \n","  # Instantiates the model\n","  print(torch.cuda.get_device_name(0))\n","  net = Our_CNN()\n","  net = net.to(device)\n","  \n","  # Instantiates the optimizer\n","  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","  \n","  # Instantiates the cost function\n","  #cost_function = get_cost_function()\n","\n","  print('Before training:')\n","  train_loss, train_accuracy = test(net, train_loader)\n","  val_loss, val_accuracy = test(net,val_loader)\n","\n","  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","  print('\\t Val loss {:.5f}, Val accuracy {:.2f}'.format(val_loss, val_accuracy))\n","  print('-----------------------------------------------------')\n","  \n","  # Add values to plots\n","  writer.add_scalar('Loss/train_loss', train_loss, 0)\n","  writer.add_scalar('Loss/val_loss', val_loss, 0)\n","  writer.add_scalar('Accuracy/train_accuracy', train_accuracy, 0)\n","  writer.add_scalar('Accuracy/val_accuracy', val_accuracy, 0)\n","\n","  for e in range(epochs):\n","    train_loss, train_accuracy = train(net, train_loader, optimizer)\n","    val_loss, val_accuracy = test(net, val_loader)\n","    print('Epoch: {:d}'.format(e+1))\n","    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","    print('\\t Val loss {:.5f}, Val accuracy {:.2f}'.format(val_loss, val_accuracy))\n","    print('-----------------------------------------------------')\n","    \n","    # Add values to plots\n","    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n","    writer.add_scalar('Loss/val_loss', val_loss, e + 1)\n","    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n","    writer.add_scalar('Accuracy/val_accuracy', val_accuracy, e + 1)\n","\n","  print('After training:')\n","  train_loss, train_accuracy = test(net, train_loader)\n","  val_loss, val_accuracy = test(net, val_loader)\n","\n","  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","  print('\\t Val loss {:.5f}, Val accuracy {:.2f}'.format(val_loss, val_accuracy))\n","  print('-----------------------------------------------------')\n","\n","  # Closes the logger\n","  writer.close()\n","\n","main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL7OAb0Iu5G2"},"outputs":[],"source":["#MAIN PER TASK 2\n","#Ripetere da sopra e poi???"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"MAIN_CODE_UNITO.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
